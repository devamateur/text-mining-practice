{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ee948f",
   "metadata": {},
   "source": [
    "# 텍스트 전처리(Text Preprocesing)\n",
    "### 1) 정제(Cleaning): 분석에 불필요한 노이즈 제거(noise removal)  \n",
    "### &nbsp; &nbsp; ex) 불용어(stopwords) 제거 <br>\n",
    "### 2) 토큰화(Tokenization): 주어진 텍스트를 토큰으로 나누는 작업\n",
    "### &nbsp; &nbsp; ex) **단어 토큰화(word tokenization)**, 문장 토큰화 <br>\n",
    "### 3) 정규화: 같은 의미를 가진 다른 형태의 단어들을 통일시키는 작업\n",
    "### &nbsp; &nbsp; - 어간 추출(Stemming), 표제어 추출(Lemmatization)  <br>\n",
    "### 4) 품사 태깅: 토큰화한 단어에 대해 품사를 파악해 부착하는 것을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee6ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\ing06\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 영어 토큰화 - nltk 이용\n",
    "import nltk\n",
    "\n",
    "# 구텐베르크 불러오기\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "file_names = gutenberg.fileids()\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdcfbe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lightning. Enter three Witches.\n",
      "\n",
      "  1. When shall we three meet againe?\n",
      "In Thunder, Lightning, or in Raine?\n",
      "  2. When the Hurley-burley's done,\n",
      "When the Battaile's lost, and wonne\n",
      "\n",
      "   3. That will be ere the set of Sunne\n",
      "\n",
      "   1. Where the place?\n",
      "  2. Vpon the Heath\n",
      "\n",
      "   3. There to meet with Macbeth\n",
      "\n",
      "   1. I come, Gray-Malkin\n",
      "\n",
      "   All. Padock calls anon: faire is foule, and foule is faire,\n",
      "Houer through \n"
     ]
    }
   ],
   "source": [
    "# 맥베스 샘플 출력\n",
    "doc_macbeth = gutenberg.open('shakespeare-macbeth.txt').read()\n",
    "print(doc_macbeth[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77d8d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The Tragedie of Macbeth by William Shakespeare 1603]\\n\\n\\nActus Primus.', 'Scoena Prima.', 'Thunder and Lightning.', 'Enter three Witches.', '1.', 'When shall we three meet againe?', 'In Thunder, Lightning, or in Raine?', '2.', \"When the Hurley-burley's done,\\nWhen the Battaile's lost, and wonne\\n\\n   3.\", 'That will be ere the set of Sunne\\n\\n   1.', 'Where the place?', '2.', 'Vpon the Heath\\n\\n   3.', 'There to meet with Macbeth\\n\\n   1.', 'I come, Gray-Malkin\\n\\n   All.', 'Padock calls anon: faire is foule, and foule is faire,\\nHouer through']\n"
     ]
    }
   ],
   "source": [
    "# 1. 문장 토큰화 - 문장으로 토큰을 나눔\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sample = doc_macbeth[:500]\n",
    "\n",
    "print(sent_tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7026c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Thunder', 'and', 'Lightning', '.', 'Enter', 'three', 'Witches', '.', '1', '.', 'When', 'shall', 'we', 'three', 'meet', 'againe', '?', 'In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?', '2', '.', 'When', 'the', 'Hurley-burley', \"'s\", 'done', ',', 'When', 'the', 'Battaile', \"'s\", 'lost', ',', 'and', 'wonne', '3', '.', 'That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne', '1', '.', 'Where', 'the', 'place', '?', '2', '.', 'Vpon', 'the', 'Heath', '3', '.', 'There', 'to', 'meet', 'with', 'Macbeth', '1', '.', 'I', 'come', ',', 'Gray-Malkin', 'All', '.', 'Padock', 'calls', 'anon', ':', 'faire', 'is', 'foule', ',', 'and', 'foule', 'is', 'faire', ',', 'Houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "# 2. 단어 토큰화 \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(sample))\n",
    "# 괄호나 점 같은 필요없는 문자들이 같이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba6c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', 'Actus', 'Primus', 'Scoena', 'Prima', 'Thunder', 'and', 'Lightning', 'Enter', 'three', 'Witches', '1', 'When', 'shall', 'we', 'three', 'meet', 'againe', 'In', 'Thunder', 'Lightning', 'or', 'in', 'Raine', '2', 'When', 'the', 'Hurley', \"burley's\", 'done', 'When', 'the', \"Battaile's\", 'lost', 'and', 'wonne', '3', 'That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne', '1', 'Where', 'the', 'place', '2', 'Vpon', 'the', 'Heath', '3', 'There', 'to', 'meet', 'with', 'Macbeth', '1', 'I', 'come', 'Gray', 'Malkin', 'All', 'Padock', 'calls', 'anon', 'faire', 'is', 'foule', 'and', 'foule', 'is', 'faire', 'Houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 사용\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# \\w: 문자, 숫자, _를 포함 [\\w] = [0-9A-Za-z]\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "print(tokenizer.tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88add3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'Macbeth', 'William', 'Shakespeare', '1603', 'Actus', 'Primus', 'Scoena', 'Prima', 'Thunder', 'and', 'Lightning', 'Enter', 'three', 'Witches', 'When', 'shall', 'three', 'meet', 'againe', 'Thunder', 'Lightning', 'Raine', 'When', 'the', 'Hurley', \"burley's\", 'done', 'When', 'the', \"Battaile's\", 'lost', 'and', 'wonne', 'That', 'will', 'ere', 'the', 'set', 'Sunne', 'Where', 'the', 'place', 'Vpon', 'the', 'Heath', 'There', 'meet', 'with', 'Macbeth', 'come', 'Gray', 'Malkin', 'All', 'Padock', 'calls', 'anon', 'faire', 'foule', 'and', 'foule', 'faire', 'Houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")         # 3글자 이상\n",
    "print(tokenizer.tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "217675d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'Macbeth', 'William', 'Shakespeare', '1603', 'Actus', 'Primus', 'Scoena', 'Prima', 'Thunder', 'Lightning', 'Enter', 'three', 'Witches', '1', 'When', 'shall', 'three', 'meet', 'againe', 'In', 'Thunder', 'Lightning', 'Raine', '2', 'When', 'Hurley', \"burley's\", 'done', 'When', \"Battaile's\", 'lost', 'wonne', '3', 'That', 'ere', 'set', 'Sunne', '1', 'Where', 'place', '2', 'Vpon', 'Heath', '3', 'There', 'meet', 'Macbeth', '1', 'I', 'come', 'Gray', 'Malkin', 'All', 'Padock', 'calls', 'anon', 'faire', 'foule', 'foule', 'faire', 'Houer']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "token = tokenizer.tokenize(sample)\n",
    "\n",
    "result = [word for word in token if word not in english_stopwords]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906aaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
