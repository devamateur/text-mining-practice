{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ee948f",
   "metadata": {},
   "source": [
    "# 텍스트 전처리(Text Preprocesing)\n",
    "---------------------\n",
    "## 1) 정제(Cleaning): 분석에 불필요한 노이즈 제거(noise removal)  \n",
    "### &nbsp; &nbsp; ex) 불용어(stopwords) 제거 <br/>\n",
    "## 2) 토큰화(Tokenization): 주어진 텍스트를 토큰으로 나누는 작업\n",
    "### &nbsp; &nbsp; ex) **단어 토큰화(word tokenization)**, 문장 토큰화 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee6ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\ing06\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 영어 토큰화 - nltk 이용\n",
    "import nltk\n",
    "\n",
    "# 구텐베르크 불러오기\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "file_names = gutenberg.fileids()\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcfbe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lightning. Enter three Witches.\n",
      "\n",
      "  1. When shall we three meet againe?\n",
      "In Thunder, Lightning, or in Raine?\n",
      "  2. When the Hurley-burley's done,\n",
      "When the Battaile's lost, and wonne\n",
      "\n",
      "   3. That will be ere the set of Sunne\n",
      "\n",
      "   1. Where the place?\n",
      "  2. Vpon the Heath\n",
      "\n",
      "   3. There to meet with Macbeth\n",
      "\n",
      "   1. I come, Gray-Malkin\n",
      "\n",
      "   All. Padock calls anon: faire is foule, and foule is faire,\n",
      "Houer through \n"
     ]
    }
   ],
   "source": [
    "# 맥베스 샘플 출력\n",
    "doc_macbeth = gutenberg.open('shakespeare-macbeth.txt').read()\n",
    "print(doc_macbeth[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77d8d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[The Tragedie of Macbeth by William Shakespeare 1603]\\n\\n\\nActus Primus.', 'Scoena Prima.', 'Thunder and Lightning.', 'Enter three Witches.', '1.', 'When shall we three meet againe?', 'In Thunder, Lightning, or in Raine?', '2.', \"When the Hurley-burley's done,\\nWhen the Battaile's lost, and wonne\\n\\n   3.\", 'That will be ere the set of Sunne\\n\\n   1.', 'Where the place?', '2.', 'Vpon the Heath\\n\\n   3.', 'There to meet with Macbeth\\n\\n   1.', 'I come, Gray-Malkin\\n\\n   All.', 'Padock calls anon: faire is foule, and foule is faire,\\nHouer through']\n"
     ]
    }
   ],
   "source": [
    "# 1. 문장 토큰화 - 문장으로 토큰을 나눔\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sample = doc_macbeth[:500]\n",
    "\n",
    "print(sent_tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7026c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Thunder', 'and', 'Lightning', '.', 'Enter', 'three', 'Witches', '.', '1', '.', 'When', 'shall', 'we', 'three', 'meet', 'againe', '?', 'In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?', '2', '.', 'When', 'the', 'Hurley-burley', \"'s\", 'done', ',', 'When', 'the', 'Battaile', \"'s\", 'lost', ',', 'and', 'wonne', '3', '.', 'That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne', '1', '.', 'Where', 'the', 'place', '?', '2', '.', 'Vpon', 'the', 'Heath', '3', '.', 'There', 'to', 'meet', 'with', 'Macbeth', '1', '.', 'I', 'come', ',', 'Gray-Malkin', 'All', '.', 'Padock', 'calls', 'anon', ':', 'faire', 'is', 'foule', ',', 'and', 'foule', 'is', 'faire', ',', 'Houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "# 2. 단어 토큰화 \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(sample))\n",
    "# 괄호나 점 같은 필요없는 문자들이 같이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba6c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', 'Actus', 'Primus', 'Scoena', 'Prima', 'Thunder', 'and', 'Lightning', 'Enter', 'three', 'Witches', '1', 'When', 'shall', 'we', 'three', 'meet', 'againe', 'In', 'Thunder', 'Lightning', 'or', 'in', 'Raine', '2', 'When', 'the', 'Hurley', \"burley's\", 'done', 'When', 'the', \"Battaile's\", 'lost', 'and', 'wonne', '3', 'That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne', '1', 'Where', 'the', 'place', '2', 'Vpon', 'the', 'Heath', '3', 'There', 'to', 'meet', 'with', 'Macbeth', '1', 'I', 'come', 'Gray', 'Malkin', 'All', 'Padock', 'calls', 'anon', 'faire', 'is', 'foule', 'and', 'foule', 'is', 'faire', 'Houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 사용\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# \\w: 문자, 숫자, _를 포함 [\\w] = [0-9A-Za-z]\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "print(tokenizer.tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88add3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'Macbeth', 'William', 'Shakespeare', '1603', 'Actus', 'Primus', 'Scoena', 'Prima', 'Thunder', 'and', 'Lightning', 'Enter', 'three', 'Witches', 'When', 'shall', 'three', 'meet', 'againe', 'Thunder', 'Lightning', 'Raine', 'When', 'the', 'Hurley', \"burley's\", 'done', 'When', 'the', \"Battaile's\", 'lost', 'and', 'wonne', 'That', 'will', 'ere', 'the', 'set', 'Sunne', 'Where', 'the', 'place', 'Vpon', 'the', 'Heath', 'There', 'meet', 'with', 'Macbeth', 'come', 'Gray', 'Malkin', 'All', 'Padock', 'calls', 'anon', 'faire', 'foule', 'and', 'foule', 'faire', 'Houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\")         # 3글자 이상\n",
    "print(tokenizer.tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "217675d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'Macbeth', 'William', 'Shakespeare', '1603', 'Actus', 'Primus', 'Scoena', 'Prima', 'Thunder', 'Lightning', 'Enter', 'three', 'Witches', '1', 'When', 'shall', 'three', 'meet', 'againe', 'In', 'Thunder', 'Lightning', 'Raine', '2', 'When', 'Hurley', \"burley's\", 'done', 'When', \"Battaile's\", 'lost', 'wonne', '3', 'That', 'ere', 'set', 'Sunne', '1', 'Where', 'place', '2', 'Vpon', 'Heath', '3', 'There', 'meet', 'Macbeth', '1', 'I', 'come', 'Gray', 'Malkin', 'All', 'Padock', 'calls', 'anon', 'faire', 'foule', 'foule', 'faire', 'Houer']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "token = tokenizer.tokenize(sample)\n",
    "\n",
    "result = [word for word in token if word not in english_stopwords]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e830e",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## 3) 정규화: 같은 의미를 가진 다른 형태의 단어들을 통일시키는 작업\n",
    "### &nbsp; &nbsp; - 어간 추출(Stemming): 단어의 형태가 변화할 때, 변하지 않는 부분(어간)을 추출\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; 따라서 어간 추출 결과는 사전에 등록되지 않은 단어일 확률이 높음\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; ex) 가다, 간다, 갔다 -> '가' &nbsp; &nbsp; &nbsp; 작다, 작고, 작으니 -> '작' \n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### &nbsp; &nbsp; - 표제어 추출(Lemmatization): 단어를 기본형으로 변환하는 것\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; ex) cooking -> cook(동사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06043246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "# 어간 추출(Stemming)\n",
    "# 1. 포터 스테머\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7569d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화:  ['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']', 'Actus', 'Primus', '.', 'Scoena', 'Prima', '.', 'Thunder', 'and', 'Lightning', '.', 'Enter', 'three', 'Witches', '.', '1', '.', 'When', 'shall', 'we', 'three', 'meet', 'againe', '?', 'In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?', '2', '.', 'When', 'the', 'Hurley-burley', \"'s\", 'done', ',', 'When', 'the', 'Battaile', \"'s\", 'lost', ',', 'and', 'wonne', '3', '.', 'That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne', '1', '.', 'Where', 'the', 'place', '?', '2', '.', 'Vpon', 'the', 'Heath', '3', '.', 'There', 'to', 'meet', 'with', 'Macbeth', '1', '.', 'I', 'come', ',', 'Gray-Malkin', 'All', '.', 'Padock', 'calls', 'anon', ':', 'faire', 'is', 'foule', ',', 'and', 'foule', 'is', 'faire', ',', 'Houer', 'through']\n",
      "어간 추출:  ['[', 'the', 'tragedi', 'of', 'macbeth', 'by', 'william', 'shakespear', '1603', ']', 'actu', 'primu', '.', 'scoena', 'prima', '.', 'thunder', 'and', 'lightn', '.', 'enter', 'three', 'witch', '.', '1', '.', 'when', 'shall', 'we', 'three', 'meet', 'again', '?', 'in', 'thunder', ',', 'lightn', ',', 'or', 'in', 'rain', '?', '2', '.', 'when', 'the', 'hurley-burley', \"'s\", 'done', ',', 'when', 'the', 'battail', \"'s\", 'lost', ',', 'and', 'wonn', '3', '.', 'that', 'will', 'be', 'ere', 'the', 'set', 'of', 'sunn', '1', '.', 'where', 'the', 'place', '?', '2', '.', 'vpon', 'the', 'heath', '3', '.', 'there', 'to', 'meet', 'with', 'macbeth', '1', '.', 'i', 'come', ',', 'gray-malkin', 'all', '.', 'padock', 'call', 'anon', ':', 'fair', 'is', 'foul', ',', 'and', 'foul', 'is', 'fair', ',', 'houer', 'through']\n"
     ]
    }
   ],
   "source": [
    "sample = doc_macbeth[:500]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sample)\n",
    "print(\"토큰화: \", tokens)\n",
    "stem = [stemmer.stem(token) for token in tokens]\n",
    "print(\"어간 추출: \", stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6d7440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "# 2. 랭카스터 스테머\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출(Lemmatization)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking'), pos='v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
