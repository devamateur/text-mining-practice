{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "059a6ea0",
   "metadata": {},
   "source": [
    "# 동적 토픽 모델링(Dynamic Topic Modeling)\n",
    "- 앞에서 수행한 토픽 트렌드 분석은 토픽들이 고정되어 있었음\n",
    "- 시간이 지나면서 토픽 역시 변하기 때문에 이를 이용한 트렌드 분석이 필요\n",
    "<br/>\n",
    "\n",
    "- **동적 토픽 모델링**: 이전 토픽을 반영해 다음 시간의 토픽을 추출하는 것 <br/>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; 즉, **어떤 시점 t에서의 α, β가 이전 시점인 t-1에서의 α, β에 의해 결정된다고 가정** <br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "## 동적 토픽 모델링 원리\n",
    "<img src=\"https://lyusungwon.github.io/assets/images/dynamic_topic_model.png\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "그림의 각 열은 LDA 구조, **어떤 시점 t에서의 α, β가 이전 시점인 t-1에서의 α, β에 의해 결정된다고 가정** <br/> <br/>\n",
    "\n",
    "<수식> αt | αt−1 ∼ N (αt−1,δ2I), &nbsp; &nbsp; βt | βt−1 ∼ N (βt−1,σ2I) <br/>\n",
    "-> αt와 βt는 이전 시점의 αt−1, βt−1를 평균으로 하는 정규분포로부터 샘플링됨을 의미 <br/> <br/> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6317e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2017-08\n",
      "1    2017-08\n",
      "2    2017-08\n",
      "Name: time, dtype: object\n",
      "[83, 906, 298, 1446, 926, 1514, 1304, 1097, 1217, 1301, 1246, 1291, 1390, 1155, 1277, 1187, 439] 18077\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/petition_sampled.csv\")\n",
    "df_sorted = df.sort_values(by='start')     # 청원 날짜 순으로 정렬\n",
    "\n",
    "# time slice 생성\n",
    "df_sorted['time'] = df_sorted['start'].map(lambda x: x[:7])      # 월까지만 추출해서 time 컬럼 생성\n",
    "print(df_sorted['time'][:3]) \n",
    "\n",
    "# 월별 문서 개수를 time_slice에 저장\n",
    "time_slice = list(df_sorted['time'].value_counts().sort_index())\n",
    "print(time_slice, sum(time_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84eb215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-08', '2017-09', '2017-10', '2017-11', '2017-12', '2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12'] [83, 906, 298, 1446, 926, 1514, 1304, 1097, 1217, 1301, 1246, 1291, 1390, 1155, 1277, 1187, 439]\n",
      "2017-08      83\n",
      "2017-09     906\n",
      "2017-10     298\n",
      "2017-11    1446\n",
      "2017-12     926\n",
      "2018-01    1514\n",
      "2018-02    1304\n",
      "2018-03    1097\n",
      "2018-04    1217\n",
      "2018-05    1301\n",
      "2018-06    1246\n",
      "2018-07    1291\n",
      "2018-08    1390\n",
      "2018-09    1155\n",
      "2018-10    1277\n",
      "2018-11    1187\n",
      "2018-12     439\n",
      "Name: time, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 시간 구간을 리스트로 작성\n",
    "time_tag = sorted(list(set(df_sorted['time'])))\n",
    "\n",
    "# 제대로 대응되는지 확인\n",
    "print(time_tag, time_slice)\n",
    "print(df_sorted['time'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2609f6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Number of initial unique words in documents: 36344\n",
      "#Number of unique words after removing rare and common words: 2000\n",
      "#Number of unique tokens: 2000\n",
      "#Number of documents: 18077\n"
     ]
    }
   ],
   "source": [
    "# dictionary, corpus 생성\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "t = Okt()\n",
    "\n",
    "def tokenizer(doc): #명사만 사용\n",
    "    return [token for token in t.nouns(doc) if len(token) > 1]\n",
    "\n",
    "#청원 텍스트를 토큰화\n",
    "texts = [tokenizer(news) for news in df_sorted['content']]\n",
    "\n",
    "# 토큰화 결과로부터 dictionay 생성\n",
    "dictionary = Dictionary(texts) \n",
    "print('#Number of initial unique words in documents:', len(dictionary))\n",
    "\n",
    "# 문서 빈도수가 너무 적거나 높은 단어를 필터링하고 특성을 단어의 빈도 순으로 선택\n",
    "dictionary.filter_extremes(keep_n = 2000, no_below=5, no_above=0.5)\n",
    "print('#Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "# 카운트 벡터로 변환 -> corpus 생성\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print('#Number of unique tokens: %d' % len(dictionary))\n",
    "print('#Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LdaSeqModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
