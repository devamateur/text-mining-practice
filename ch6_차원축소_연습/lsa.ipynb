{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c4b01f",
   "metadata": {},
   "source": [
    "## LSA(Latent Semantic Analysis, 잠재 의미 분석)\n",
    "- 문서의 잠재된 의미 분석 및 단어에 잠재된 의미 분석 가능\n",
    "- 의미는 문서와 단어를 연결하는 매개체, 축소된 차원이 이 역할?\n",
    "- 절단된 SVD(Truncated SVD)로 구현됨\n",
    "<br/>\n",
    "\n",
    "**SVD(Singular Value Decomposition):** 특이값 분해, m x n 크기 행렬을 세 개 행렬의 곱으로 분해하는 것 <br/><br/>\n",
    "$$ X = UΣV^T $$ <br/>\n",
    "``U와 V: m x m, n x n 크기를 갖는 직교행렬 \n",
    "Σ: m x n 크기의 대각행렬 ``\n",
    "\n",
    "분해된 세 행렬을 다시 곱해 원래 데이터를 복원할 수 있음 <br/>\n",
    "단, 절단된 SVD에서는 완전한 복원이 불가능, 최대한 유사하게\n",
    "<br/>\n",
    "\n",
    "[SVD 관련 링크](https://wikidocs.net/24949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5eb7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20뉴스그룹 데이터 불러오기\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# train\n",
    "news_train = fetch_20newsgroups(subset='train',\n",
    "                                remove=('headers', 'footers', 'quotes'),\n",
    "                                categories=categories)\n",
    "\n",
    "# test\n",
    "news_test = fetch_20newsgroups(subset='test',\n",
    "                               remove=('headers', 'footers', 'quotes'),\n",
    "                               categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0b3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words('english')   # 불용어\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# train/test split\n",
    "X_train = news_train.data\n",
    "y_train = news_train.target\n",
    "\n",
    "X_test = news_test.data\n",
    "y_test = news_test.target\n",
    "\n",
    "# 토큰화\n",
    "reg = RegexpTokenizer(\"[\\w']{3,}\")\n",
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05664b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from pca.ipynb\n",
      "# Train score: 0.962\n",
      "# Test score: 0.761\n",
      "Original Tfidf matrix shape:  (2034, 20085)\n",
      "PCA Converted matrix shape:  (2034, 2000)\n",
      "Sum of explained variance ratio: 1.000\n",
      "# Train score: 0.962\n",
      "# Test score: 0.761\n",
      "# Train score: 0.790\n",
      "# Test score: 0.718\n",
      "# Used features: 321 out of  (2034, 20085)\n",
      "PCA Converted matrix shape:  (2034, 321)\n",
      "Sum of explained variance ratio: 0.437\n",
      "# Train score: 0.875\n",
      "# Test score: 0.751\n"
     ]
    }
   ],
   "source": [
    "# pca에서 토크나이저 임포트\n",
    "import import_ipynb\n",
    "from pca import tokenizer\n",
    "\n",
    "# tfidf\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfe2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Converted X shape:  (2034, 2000)\n",
      "Sum of explained variance ratio: 1.000\n"
     ]
    }
   ],
   "source": [
    "# LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# pca과 마찬가지로 차원의 개수 2000개로 지정\n",
    "svd = TruncatedSVD(n_components=2000, random_state=7)\n",
    "X_train_lsa = svd.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = svd.transform(X_test_tfidf)\n",
    "\n",
    "print(\"LSA Converted X shape: \", X_train_lsa.shape)\n",
    "\n",
    "print(\"Sum of explained variance ratio: {:.3f}\".format(svd.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b096fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train score: 0.962\n",
      "# Test score: 0.761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_lsa, y_train)\n",
    "\n",
    "print(\"# Train score: {:.3f}\".format(lr.score(X_train_lsa, y_train)))\n",
    "print(\"# Test score: {:.3f}\".format(lr.score(X_test_lsa, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2834019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Converted X shape:  (2034, 100)\n",
      "Sum of explained variance ratio: 0.209\n",
      "# Train score: 0.810\n",
      "# Test score: 0.745\n"
     ]
    }
   ],
   "source": [
    "# 100개 차원으로 축소\n",
    "svd = TruncatedSVD(n_components=100, random_state=1)\n",
    "X_train_lsa = svd.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = svd.transform(X_test_tfidf)\n",
    "\n",
    "print(\"LSA Converted X shape: \", X_train_lsa.shape)\n",
    "\n",
    "print(\"Sum of explained variance ratio: {:.3f}\".format(svd.explained_variance_ratio_.sum()))\n",
    "\n",
    "lr.fit(X_train_lsa, y_train)\n",
    "\n",
    "print(\"# Train score: {:.3f}\".format(lr.score(X_train_lsa, y_train)))\n",
    "print(\"# Test score: {:.3f}\".format(lr.score(X_test_lsa, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1dbafc",
   "metadata": {},
   "source": [
    "### LSA를 이용한 의미 기반 유사도 계산\n",
    "\n",
    "LSA에서 축소된 차원은 잠재된 의미를 보여준다고 할 수 있음 <br/>\n",
    "앞서 4장에서 했던 것처럼 문서를 벡터로 변환한 뒤 유사도 계산하는 것이 가능 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cd3914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 카테고리:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "첫 문서 카테고리:  1\n",
      "Top 20 유사도(lsa):\n",
      " [1.0, 0.74, 0.74, 0.72, 0.7, 0.7, 0.69, 0.67, 0.66, 0.65, 0.65, 0.65, 0.63, 0.62, 0.62, 0.62, 0.57, 0.57, 0.55, 0.54, 0.54, 0.54, 0.53, 0.53, 0.52, 0.5, 0.5, 0.48, 0.48, 0.48]\n",
      "Top 20 유사도의 인덱스(lsa):\n",
      " [   0 1957 1674  501 1995 1490  790 1902 1575 1209 1728  892 1892  998\n",
      " 1038 1826 1290 1089  867  151 1691 1029   25  651  783  864  874  897\n",
      " 1724  942]\n",
      "Top 20 유사 뉴스의 카테고리(lsa):\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도\n",
    "# 첫번째 문서와 전체 문서와의 유사도를 계산\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"전체 카테고리: \", news_train.target_names)\n",
    "print(\"첫 문서 카테고리: \", y_train[0])\n",
    "\n",
    "\n",
    "sim = cosine_similarity([X_train_lsa[0]], X_train_lsa)\n",
    "\n",
    "# 유사도가 높은 30개 문서의 인덱스\n",
    "print(\"Top 20 유사도(lsa):\\n\", sorted(sim[0].round(2), reverse=True)[:30])\n",
    "sim_index = (-sim[0]).argsort()[:30]\n",
    "print(\"Top 20 유사도의 인덱스(lsa):\\n\", sim_index)\n",
    "sim_labels = [y_train[i] for i in sim_index]\n",
    "print(\"Top 20 유사 뉴스의 카테고리(lsa):\\n\", sim_labels)\n",
    "\n",
    "# 1개 문서를 제외한 모든 문서가 첫번째 문서와 같은 카테고리(comp.graphics)임을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c71ebcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 유사도(tfidf):\n",
      " [1.0, 0.3, 0.22, 0.21, 0.19, 0.19, 0.19, 0.17, 0.16, 0.16, 0.16, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.13, 0.13, 0.13, 0.13, 0.13]\n",
      "Top 20 유사도의 인덱스(tfidf):\n",
      " [   0 1575 1892 1490  501 1290 1013  998 1636 1705 1995 1957 1664  651\n",
      " 1038  429 1089 1209 1728 1803 1724  783  867 1578 1902  169  715  499\n",
      "  697  537]\n",
      "Top 20 유사 뉴스의 카테고리(tfidf):\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# tfidf\n",
    "sim = cosine_similarity(X_train_tfidf[0], X_train_tfidf)\n",
    "\n",
    "# 유사도가 높은 30개 문서의 인덱스\n",
    "print(\"Top 20 유사도(tfidf):\\n\", sorted(sim[0].round(2), reverse=True)[:30])    # 내림차순\n",
    "sim_index = (-sim[0]).argsort()[:30]     # argsort(): 배열을 정렬하는 인덱스 출력, 여기서는 내림차순 인덱스를 출력\n",
    "print(\"Top 20 유사도의 인덱스(tfidf):\\n\", sim_index)\n",
    "sim_labels = [y_train[i] for i in sim_index]\n",
    "print(\"Top 20 유사 뉴스의 카테고리(tfidf):\\n\", sim_labels)\n",
    "\n",
    "# 유사도 값이 lsa보다 낮음을 알 수 있음\n",
    "# -> 차원이 lsa보다 상대적으로 커서 차원의 저주가 적용됐기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fc9a1",
   "metadata": {},
   "source": [
    "### 토픽 모델링\n",
    "잠재된 의미와 연결된 단어들을 통해 문서를 이루고 있는 잠재 의미를 파악할 수 있음 <br/>\n",
    "이런 잠재 의미를 '토픽'이라고 하고, 분석 방법을 토픽 모델링이라고 함 <br/> <br/>\n",
    "최근의 토픽 모델링은 대부분 LDA(Latent Dirichlet Allocation)을 사용하지만 <br/>\n",
    "LDA가 나오기 전에는 LSA가 토픽 모델링에 사용되기도 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed5ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Converted X shape:  (2034, 10)\n",
      "Sum of explained variance ratio: 0.045\n"
     ]
    }
   ],
   "source": [
    "# 차원을 10개로 축소\n",
    "svd = TruncatedSVD(n_components=10, random_state=1)\n",
    "X_train_lsa = svd.fit_transform(X_train_tfidf)\n",
    "X_test_lsa = svd.transform(X_test_tfidf)\n",
    "\n",
    "print(\"LSA Converted X shape: \", X_train_lsa.shape)\n",
    "\n",
    "print(\"Sum of explained variance ratio: {:.3f}\".format(svd.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79873860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  ['would', 'one', 'god', 'think', 'use', 'peopl', 'know', 'like', 'say', 'space'] [0.17, 0.15, 0.14, 0.13, 0.13, 0.13, 0.12, 0.12, 0.11, 0.11]\n",
      "Topic 2:  ['file', 'imag', 'thank', 'program', 'graphic', 'space', 'format', 'use', 'color', 'ftp'] [0.22, 0.18, 0.17, 0.16, 0.16, 0.15, 0.12, 0.1, 0.1, 0.1]\n",
      "Topic 3:  ['space', 'orbit', 'nasa', 'launch', 'shuttl', 'satellit', 'year', 'moon', 'lunar', 'cost'] [0.4, 0.17, 0.17, 0.17, 0.1, 0.1, 0.1, 0.1, 0.1, 0.09]\n",
      "Topic 4:  ['moral', 'object', 'system', 'valu', 'goal', 'think', 'anim', 'absolut', 'natur', 'defin'] [0.53, 0.34, 0.2, 0.12, 0.1, 0.1, 0.09, 0.08, 0.08, 0.07]\n",
      "Topic 5:  ['ico', 'bobb', 'tek', 'beauchain', 'bronx', 'manhattan', 'sank', 'queen', 'vice', 'blew'] [0.23, 0.23, 0.23, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22]\n",
      "Topic 6:  ['god', 'file', 'imag', 'object', 'moral', 'exist', 'space', 'format', 'system', 'color'] [0.36, 0.23, 0.23, 0.17, 0.16, 0.14, 0.14, 0.13, 0.1, 0.08]\n",
      "Topic 7:  ['file', 'islam', 'imag', 'cview', 'use', 'format', 'color', 'muslim', 'religion', 'peopl'] [0.26, 0.21, 0.2, 0.13, 0.12, 0.1, 0.1, 0.1, 0.09, 0.08]\n",
      "Topic 8:  ['post', 'file', 'space', 'islam', 'read', 'cview', 'format', 'articl', 'group', 'moral'] [0.3, 0.29, 0.22, 0.14, 0.12, 0.11, 0.11, 0.1, 0.1, 0.09]\n",
      "Topic 9:  ['christian', 'graphic', 'imag', 'jesu', 'book', 'data', 'group', 'softwar', 'law', 'code'] [0.25, 0.21, 0.18, 0.16, 0.13, 0.1, 0.1, 0.09, 0.09, 0.08]\n",
      "Topic 10:  ['exist', 'atheism', 'atheist', 'graphic', 'delet', 'post', 'god', 'one', 'group', 'newsgroup'] [0.18, 0.16, 0.16, 0.16, 0.14, 0.13, 0.13, 0.13, 0.12, 0.08]\n"
     ]
    }
   ],
   "source": [
    "# 토픽별로 비중이 높은 단어 10개 출력\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "def get_topics(model, feature_names, n=10):\n",
    "    for idx, topic in enumerate(model.components_):   # components_: 각 단어에 대해 축소된 차원, 즉 의미의 비중\n",
    "        print(\"Topic %d: \"%(idx+1), [feature_names[i] for i in topic.argsort()[:-n-1:-1]], # 처음부터 끝까지 내림차순으로(-1) 가져오기\n",
    "              [topic[i].round(2) for i in topic.argsort()[:-n-1:-1]])    # topic\n",
    "get_topics(svd, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fe933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
